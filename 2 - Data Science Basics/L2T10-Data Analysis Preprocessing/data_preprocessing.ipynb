{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWmFBD46S5h_"
   },
   "source": [
    "## Data Preprocessing Examples\n",
    "\n",
    " In this task, we will work on handling missing data as well as data standardisation and normalisation. Data can have missing values for a number of reasons, such as observations that were not collected or data corruption. We will discuss some general considerations for missing data, discuss how pandas chooses to represent it, and demonstrate some built-in pandas tools for handling missing data in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAWxBtzoS5h1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scaling modules\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensures the same random data is used each time you execute the code\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "error",
     "timestamp": 1713260437415,
     "user": {
      "displayName": "Bianca Cherkaev",
      "userId": "05472675678669700855"
     },
     "user_tz": -120
    },
    "id": "btxBrfKC0Rp9",
    "outputId": "9a0168b8-0edb-4098-b3ff-4e28513254ad"
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('balance_missing.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1KHwSe94XT-"
   },
   "source": [
    "## Working with missing data\n",
    "### Take a first look at the data\n",
    "This should always be the first step during any analysis. Depending on data sources, missing data are identified differently. Pandas identifies missing values as NaN. However, missing values can appear as a question mark (?) or a zero (0) or minus one (-1) or a blank. So it is important to view and understand your data before anything else.\n",
    "\n",
    "We are going to use the dataset `balance_missing.txt`. From the first five observations, we can see there are several missing values. They have been represented as NaN.\n",
    "\n",
    "To start, let’s check the dimensions of the DataFrame to understand its size and then preview the first few rows to get a sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XG8G6QOo0RqA",
    "outputId": "e7c19456-0dd9-4aa2-9f53-a87a9a12f84b"
   },
   "outputs": [],
   "source": [
    "# Return the number of rows and columns of the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2S5AjI5S5iI"
   },
   "source": [
    "How many NaN values can you point out?\n",
    "\n",
    "Now that we have seen that there are missing values, we need to get a sense of how many missing values exist in the whole dataset. We are going to find how many missing values we have in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2LlSluNS5iL",
    "outputId": "ace0cd28-aa52-4f2d-cf21-11e726e275e0"
   },
   "outputs": [],
   "source": [
    "# Get the number of missing data values per column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Look at the number of missing values in the first ten columns\n",
    "missing_values_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2TWaWbbS5iO"
   },
   "source": [
    "To better understand how much data is missing, we can calculate the percentage of all the missing values. If we get a very high percentage, we may be unable to use the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvIlLTN9S5iP",
    "outputId": "0da32cb3-6354-42e5-91ad-9f96a7a75867"
   },
   "outputs": [],
   "source": [
    "# Total number of missing values\n",
    "total_cells = np.prod(df.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "\n",
    "# Percent of data that is missing\n",
    "(total_missing/total_cells) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIDPTi5uS5iR"
   },
   "source": [
    "Only 1.77% of the values are missing.\n",
    "\n",
    "### Drop missing values\n",
    "\n",
    "One option to deal with missing values is to remove any rows or columns that contain missing values. This strategy is not recommended for datasets with important data, sensitive data, or few observations. It's usually worth taking the time to go through your data and interrogate all the columns with missing values to get a proper understanding of your dataset.\n",
    "\n",
    "If you're sure you want to drop rows with missing values, pandas has a handy function, `dropna()` to do this. Let's try it out on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDOexSCHS5iS",
    "outputId": "ba4fabfa-c2a5-4452-97f3-9f1a8231f28c"
   },
   "outputs": [],
   "source": [
    "# Create a temporary DataFrame\n",
    "temp_df = df\n",
    "\n",
    "# Remove all the rows that contain a missing value.\n",
    "temp_df.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK_poFuQS5iT"
   },
   "source": [
    "As you can see, there are no longer NaN values in the first five observations as these have been dropped from the DataFrame. In fact, all observations with a NaN value in any of the columns have been dropped. Below you can see the number of rows has been reduced to 331 compared to 400 in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYcCicyw0RqH",
    "outputId": "b5fd769b-6e90-4de2-fd91-611cd5ad3f07"
   },
   "outputs": [],
   "source": [
    "temp_df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G2aTx2e0RqI"
   },
   "source": [
    "The above procedure removed observations. Now let us look at removing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdg-iRJJS5iV",
    "outputId": "be354658-67b3-4e87-dbb9-ebf887ef79f4"
   },
   "outputs": [],
   "source": [
    "# Create a temporary Dataframe\n",
    "temp_df = df\n",
    "\n",
    "# Remove all columns with at least one missing value\n",
    "columns_with_na_dropped = temp_df.dropna(axis=1)\n",
    "columns_with_na_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZgpkni9S5iX"
   },
   "source": [
    "Looks like all the columns had at least one missing value except the \"Balance\" column. Losing all columns but one reduces the amount of data you have drastically. This is one of the reasons why removing observations or columns with missing data may not be a good solution. We'll explore other options next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lz4q1-x3S5iY"
   },
   "source": [
    "### Filling in missing values automatically\n",
    "\n",
    "We can use the pandas `fillna()` function to fill in missing values in a dataframe. One option is to specify what we want the NaN values to be replaced with. Here, we can replace all NaN with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggkh0Kk1S5iZ",
    "outputId": "9cfd5e94-9069-4154-de91-77cfeefbfad2"
   },
   "outputs": [],
   "source": [
    "# Create a temporary DataFrame\n",
    "temp_df = df\n",
    "\n",
    "# Replace all NA's with 0\n",
    "temp_df.fillna(0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTm8DwAzS5ic"
   },
   "source": [
    "Another option is to replace the missing values with the values that come just before or just after it in the same column. This can be used in datasets where the observations are in some sorted or logical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swR7FThkS5ie",
    "outputId": "7b9724ea-9722-4ffd-beab-a238e22e8a22"
   },
   "outputs": [],
   "source": [
    "# Create a temporary dataframe\n",
    "temp_df = df\n",
    "\n",
    "# Replace all NaN's with the value that comes directly after it in the same column,\n",
    "# then replace all the remaining NaN's with 0\n",
    "\n",
    "# Use bfill() method to fill NaN values with the next value in the column\n",
    "temp_df = temp_df.bfill(axis=0)\n",
    "\n",
    "# Then fill any remaining NaN values with 0\n",
    "temp_df = temp_df.fillna(0)\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH7VLp2Yiru6"
   },
   "source": [
    "### Feature Scaling Recap\n",
    "\n",
    "Feature scaling is a technique used to transform the values of features so that they lie within a similar range or scale. This process is important in data preprocessing and analysis because it ensures that all features are comparable and contributes equally to subsequent analyses. The two main methods of feature scaling are **standardisation** and **normalisation**.\n",
    "\n",
    "#### Standardisation vs. Normalisation: What’s the Difference?\n",
    "\n",
    "Standardisation and normalisation both adjust the scale of features, but they do so in different ways:\n",
    "\n",
    "- **Standardisation** (also known as Z-score normalisation) transforms the data to have a mean of 0 and a standard deviation of 1. This method centres the data distribution around zero and scales it to have a unit variance. Standardisation is useful when the data has different scales or when you want to bring features to a common scale while preserving the distribution’s shape. It is beneficial when dealing with data that might have outliers or different units of measurement.\n",
    "\n",
    "- **Normalisation** (also known as Min-Max scaling) adjusts the data to fit within a specific range, typically between 0 and 1. This technique rescales the data based on the minimum and maximum values of each feature. Normalisation is useful when you need to ensure that all features contribute equally to the analysis, especially when the data has widely varying ranges or units. It is particularly helpful when features need to be bounded within a specific range for practical reasons, such as visualisation or integration with other systems.\n",
    "\n",
    "#### When to Use Which Scaling Method:\n",
    "\n",
    "- **Standardisation**:\n",
    "  - Use when the features have different units or scales and you want to bring them to a common scale while maintaining their distribution properties.\n",
    "  - Appropriate for data with outliers or varying distributions, as it is less sensitive to extreme values compared to normalisation.\n",
    "  - Use when the data follows a Gaussian or Normal distribution.\n",
    "\n",
    "\n",
    "- **Normalisation**:\n",
    "  - Use when you need to scale features to a fixed range, such as [0, 1], to ensure that all features contribute equally to the analysis.\n",
    "  - Ideal when features have different units or ranges, and you want to bound the data within a specific range for consistency.\n",
    "\n",
    "In summary, **standardisation** adjusts data to have a mean of 0 and a standard deviation of 1, making it suitable for data with different scales and distributions. **Normalisation** scales data to a fixed range, typically [0, 1], ensuring that features are on a common scale without altering their distribution shape. For a deeper understanding of when to normalise or standardise data, you can refer to this **[article](https://www.secoda.co/learn/when-to-normalize-or-standardize-data)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYvc-vLM0RqL"
   },
   "source": [
    "#### Standarisation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code example below starts by generating 1000 random data points from a **[normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)** with a mean of 10 and a standard deviation of 5, reshaping this data into a **[column vector](https://www.statlect.com/matrix-algebra/vectors-and-matrices)**. It then creates a `StandardScaler` object for standardisation. The scaler is applied to the data, transforming it so that the values have a mean of 0 and a standard deviation of 1. The code then plots two histograms side by side: the first shows the distribution of the original data, while the second displays the distribution after standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdwz053K0RqN",
    "outputId": "053b9bfd-7069-437f-ef7f-6554757d2c43"
   },
   "outputs": [],
   "source": [
    "# Generate 1000 data points randomly drawn from a normal distribution\n",
    "original_data = np.random.normal(loc=10, scale=5, size=1000).reshape(-1, 1)\n",
    "\n",
    "# Create the StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_data = scaler.fit_transform(original_data)\n",
    "\n",
    "# Plot the original and scaled data to compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.histplot(original_data, ax=ax[0], kde=True)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "\n",
    "sns.histplot(scaled_data, ax=ax[1], kde=True)\n",
    "ax[1].set_title(\"Scaled Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN3tFUD80RqO"
   },
   "source": [
    "Notice that the standardised data has a mean of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iylvbYk70RqO"
   },
   "source": [
    "#### Normalisation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code example below first generates 1000 random data points from an **[exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution)** and reshapes it into a **[column vector](https://www.statlect.com/matrix-algebra/vectors-and-matrices)**. It then creates an instance of `MinMaxScaler`, which is used to normalise the data. The scaler is fitted to the original data and subsequently transforms it, scaling the values to a range between 0 and 1. Finally, the code plots two histograms side by side: the first histogram shows the distribution of the original exponential data, while the second histogram displays the distribution of the data after normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qySQ1op2S5ig",
    "outputId": "68907147-de88-4fe6-9f88-3c2647945326"
   },
   "outputs": [],
   "source": [
    "# Generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size=1000).reshape(-1, 1)\n",
    "\n",
    "# Create the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_data = scaler.fit_transform(original_data)\n",
    "\n",
    "# Plot the original and scaled data to compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "sns.histplot(original_data, ax=ax[0], kde=True)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "\n",
    "sns.histplot(scaled_data, ax=ax[1], kde=True)\n",
    "ax[1].set_title(\"Scaled Data\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaYCOXdsS5ih"
   },
   "source": [
    "Notice that, instead of ranging from 0 to 7, it now ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYmhDLat0RqP"
   },
   "source": [
    "## Practice  \n",
    "\n",
    "Practice handing missing data with the \"countries.csv\" dataset which is derived from some World Bank data. Each column is a type of indicator for example EN.ATM.CO2E.PC is carbon dioxide emissions in metric tons per capita. These indicators are recorded for participating countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqrd_beqS5ii",
    "outputId": "19be0b4f-9976-4ef6-a5f8-9f2620dfc489"
   },
   "outputs": [],
   "source": [
    "# Load the countries data\n",
    "countries = pd.read_csv(\"countries.csv\")\n",
    "\n",
    "countries.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise the IT.CEL.SETS.P2 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e11a2tABS5ik",
    "outputId": "a82f1629-ce18-421c-ee95-0de50969bc16"
   },
   "outputs": [],
   "source": [
    "# IT.CEL.SETS.P2 is the mobile cellular subscriptions per 100 people.\n",
    "sns.histplot(countries['IT.CEL.SETS.P2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a9Aua_cS5im"
   },
   "source": [
    "#### Normalise the IT.CEL.SETS.P2 feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Extract and reshape the data to be 2D\n",
    "data_to_normalize = countries[['IT.CEL.SETS.P2']].values\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "normalized_data = scaler.fit_transform(data_to_normalize)\n",
    "\n",
    "# Assign the normalized data back to the DataFrame\n",
    "countries['IT.CEL.SETS.P2_normalized'] = normalized_data\n",
    "\n",
    "# Plot the original and normalized data to compare\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Original Data\n",
    "sns.histplot(countries['IT.CEL.SETS.P2'], ax=ax[0], kde=True)\n",
    "ax[0].set_title(\"Original Data\")\n",
    "\n",
    "# Normalized Data\n",
    "sns.histplot(countries['IT.CEL.SETS.P2_normalized'], ax=ax[1], kde=True)\n",
    "ax[1].set_title(\"Normalized Data\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qyrngFhS5ip"
   },
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kclmdBZJ0RqR"
   },
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V2ek_duS5is"
   },
   "outputs": [],
   "source": [
    "# 1. Read in store_income_data_task.csv\n",
    "\n",
    "df = pd.read_csv(\"store_income_data_task.csv\")\n",
    "\n",
    "# 2. Display the first 5 observations\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7J-wgYWS5iu"
   },
   "outputs": [],
   "source": [
    "# 3. Get the number of missing data values per column and print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIPXtprGS5iu"
   },
   "source": [
    "4. Write a note on why you think we have missing data for the following three columns: store_email,  department, and country. Remember to classify them according to the three categories(types of missingness) we analysed in the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InSxfUf40RqS"
   },
   "source": [
    "**Answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_kt5gDg0RqS"
   },
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiEvBzB8S5iv"
   },
   "source": [
    "1. For the following example, decide whether standardisation or normalisation makes more sense.\n",
    "\n",
    "  a. You want to build a **[linear regression model](http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm)** to predict someone's grades, given how much time they spend on various activities during a typical school week.  You notice that your measurements for how much time students spend studying aren't normally distributed: some students spend almost no time studying, while others study for four or more hours daily. Should you standardise or normalise this variable?  \n",
    "\n",
    "  b. You're still working with your student's grades, but you want to include information on how students perform on several fitness tests as well. You have information on how many jumping jacks and push-ups each student can complete in a minute. However, you notice that students perform far more jumping jacks than push-ups: the average for the former is 40, and for the latter only 10. Should you standardise or normalise these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "070Knn7V0Rqd"
   },
   "source": [
    "**Answer here:**  \n",
    "\n",
    "a.\n",
    "\n",
    "b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhY3QV5rS5iw"
   },
   "outputs": [],
   "source": [
    "# 2. From the countries dataset, visualise the \"EG.ELC.ACCS.ZS\" column using a histogram.\n",
    "#    Then, scale the column using the appropriate scaling method (normalisation or standardisaton).\n",
    "#    Finally, visualise the original and scaled data alongside each other.\n",
    "#    Note EG.ELC.ACCS.ZS is the percentage of the population with access to electricity.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1904059d3876957b542b45423f2a26c6c4608f5e11cc75420e543fa77f94b066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
